Understood. I will conduct an in-depth analysis of methods for imputing missing values in multiple time series, particularly macroeconomic series such as inflation, GDP, and unemployment. The focus will be on Kalman filtering techniques, Bayesian methods, and neural network approaches, incorporating explicit mathematical details and explanations.

I will compile information primarily from textbooks, educational websites, and academic research papers. I’ll let you know once the report is ready.

# Imputing Missing Values in Multiple Time Series

Imputing missing data in macroeconomic time series (e.g. inflation, GDP, unemployment) is crucial for coherent analysis and forecasting. Below we analyze three major approaches: **Kalman filtering techniques**, **Bayesian methods**, and **neural network techniques**. Each approach is described with formal notation and mathematical details, focusing on how they model the time series and handle missing observations.

## Kalman Filtering Techniques

**State-Space Model Formulation:** Kalman filter methods assume the time series arise from a state-space model. A general linear state-space model for a $p$-dimensional macroeconomic series $y_t$ (e.g. a vector containing inflation, GDP, etc.) can be written as: 

- *State equation:* $x_{t+1} = F\,x_t + w_t$, where $x_t \in \mathbb{R}^m$ is the unobserved state (e.g. underlying factors or trends) at time $t$, $F$ is the $m\times m$ state transition matrix, and $w_t \sim N(0, Q)$ is process noise (Gaussian with covariance $Q$). 
- *Observation equation:* $y_t = H\,x_t + v_t$, where $H$ is the $p\times m$ observation (design) matrix mapping states to observed series, and $v_t \sim N(0, R)$ is observation noise (with covariance $R$). 

For example, in a simple dynamic factor model, $x_t$ might represent a latent economic index and $H$ maps it to each observed series with appropriate loadings. The model can naturally handle missing data by time-indexed modification of $H$ and $R$: if some components of $y_t$ are missing, the corresponding rows of $H$ (and $R$) are omitted at time $t$ ([How to handle incomplete data in Kalman Filter? - Cross Validated](https://stats.stackexchange.com/questions/86332/how-to-handle-incomplete-data-in-kalman-filter#:~:text=%24%24%20%5Cbegin,align%7D)). In other words, we use a selection matrix $W_t$ to pick the observed subset: $y_t^* = W_t\,y_t = W_t H x_t + W_t v_t$, with $W_t$ dropping any missing entries ([How to handle incomplete data in Kalman Filter? - Cross Validated](https://stats.stackexchange.com/questions/86332/how-to-handle-incomplete-data-in-kalman-filter#:~:text=%24%24%20%5Cbegin,align%7D)). This effectively reduces the observation equation at time $t$ to only the available series.

**Kalman Filter Prediction and Update:** The Kalman filter provides recursive equations to estimate the state $x_t$ given data up to time $t$, even with intermittent missing values. Let $\hat{x}_{t|t}$ denote the estimated state at time $t$ after observing $y_t$, and $P_{t|t}$ its covariance. The **prediction (time update)** from $t-1$ to $t$ and **correction (measurement update)** at $t$ are:

- *Prediction step:* 
  $$\hat{x}_{t|t-1} = F\,\hat{x}_{t-1|t-1},$$ 
  $$P_{t|t-1} = F\,P_{t-1|t-1}\,F^T + Q.$$ 
  This projects the state and covariance forward using the transition model.

- *Update step:* When $y_t$ is observed, compute the innovation $e_t = y_t - H\,\hat{x}_{t|t-1}$ and its covariance $S_t = H\,P_{t|t-1}H^T + R$. The Kalman gain is $K_t = P_{t|t-1}H^T S_t^{-1}$. Then update the state estimate and covariance:
  $$\hat{x}_{t|t} = \hat{x}_{t|t-1} + K_t\,e_t,$$ 
  $$P_{t|t} = P_{t|t-1} - K_t\,S_t\,K_t^T.$$ 
  These equations optimally combine the prediction with new information ([smooth
](https://www.mathworks.com/help/econ/ssm.smooth.html#:~:text=,for%20period%20t%20are%20equivalent)).

If an observation is **missing at time $t$**, the Kalman filter simply skips the update. Mathematically, if $y_t$ is missing, we omit the innovation term, leaving $\hat{x}_{t|t} = \hat{x}_{t|t-1}$ and $P_{t|t} = P_{t|t-1}$ ([5.5 Missing Data | A Very Short Course on Time Series Analysis](https://bookdown.org/rdpeng/timeseriesbook/missing-data.html#:~:text=Conceptually%2C%20missing%20data%20are%20easy,to%20the%20next%20time%20point)). Intuitively, a missing data point provides no new information, so the filtered state remains at its predicted value and uncertainty does not decrease. This property is built-in: *“the Kalman filter accommodates missing data by not updating filtered state estimates corresponding to missing observations”* ([smooth
](https://www.mathworks.com/help/econ/ssm.smooth.html#:~:text=,for%20period%20t%20are%20equivalent)). (If only some components of $y_t$ are missing, we update using the observed components, as noted above with $W_t$ selection ([How to handle incomplete data in Kalman Filter? - Cross Validated](https://stats.stackexchange.com/questions/86332/how-to-handle-incomplete-data-in-kalman-filter#:~:text=%24%24%20%5Cbegin,align%7D)).)

**Expectation-Maximization (EM) for Parameter Estimation:** In many macro applications, the state-space model parameters ($F, Q, H, R$) may be unknown and need to be estimated from incomplete data. The EM algorithm can be used to find the maximum likelihood estimates of parameters in presence of missing values ([An Approach To Time Series Smoothing And Forecasting Using The Em Algorithm](https://ideas.repec.org/a/bla/jtsera/v3y1982i4p253-264.html#:~:text=,likelihood%20estimators%20for%20the%20parameters)). The EM proceeds iteratively:

- *E-step:* Given current parameter guesses, run the Kalman filter **and smoother** on the available data to compute the expected sufficient statistics, such as $E[x_t \mid \text{data}]$ and $E[x_t x_{t-1}^T \mid \text{data}]$ for all $t$. This uses the Kalman smoother (described next) to exploit all data. Essentially, we compute the conditional expectations of the "complete data" (the latent states and any missing observations) given the observed data ([An Approach To Time Series Smoothing And Forecasting Using The Em Algorithm](https://ideas.repec.org/a/bla/jtsera/v3y1982i4p253-264.html#:~:text=,likelihood%20estimators%20for%20the%20parameters)).  

- *M-step:* Update the parameter estimates by maximizing the expected log-likelihood found in the E-step. For linear Gaussian models, this has closed-form solutions. For instance, $F$ can be re-estimated by the ratio of expected cross-covariance $E[x_{t}x_{t-1}^T]$ to $E[x_{t-1}x_{t-1}^T]$, and $Q$ by the expected covariance of the state prediction errors ([An Approach To Time Series Smoothing And Forecasting Using The Em Algorithm](https://ideas.repec.org/a/bla/jtsera/v3y1982i4p253-264.html#:~:text=,likelihood%20estimators%20for%20the%20parameters)). The updated parameters are then used in the next iteration’s E-step. This EM approach (first proposed by Shumway and Stoffer) leverages the Kalman smoother to handle missing data elegantly ([An Approach To Time Series Smoothing And Forecasting Using The Em Algorithm](https://ideas.repec.org/a/bla/jtsera/v3y1982i4p253-264.html#:~:text=,likelihood%20estimators%20for%20the%20parameters)).

**Kalman Smoother and Imputation:** The Kalman **smoother** refines the state estimates by using *all* observations (both past and future of time $t$) to compute $\hat{x}_{t|T} = E[x_t \mid y_{1:T}]$. Smoothing is done via a backward recursion after the forward filter. For linear Gaussian models, the Rauch-Tung-Striebel smoother provides formulas to compute $\hat{x}_{t|T}$ and $P_{t|T}$ (the covariance) by combining the filtered estimates with future information ([State Space Models in Stan](https://jrnold.github.io/ssmodels-in-stan/filtering-and-smoothing.html#:~:text=following%20recursions%20can%20be%20used,0)) ([State Space Models in Stan](https://jrnold.github.io/ssmodels-in-stan/filtering-and-smoothing.html#:~:text=Koopman%202012%2C%20Sec%204.4.4%29%2C%20,0)). 

For imputing missing values, the smoother is invaluable: if $y_t$ was missing, one can obtain its optimal estimate by $\hat{y}_{t|T} = H\,\hat{x}_{t|T}$, i.e. plugging the smoothed state into the observation equation ([
            Dynamic principal component analysis with missing values - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC9042056/#:~:text=process%20is%20a%20linearly%20transformed,unobservable%20state%20process%20with%20noise)). Because the smoother uses information from the entire series, $\hat{y}_{t|T}$ is often more accurate than the real-time filtered estimate. In fact, one common approach to fill gaps is *Kalman smoothing imputation*: treat missing $y_t$ as unobserved and then use the smoothed output as the imputed value ([
            Dynamic principal component analysis with missing values - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC9042056/#:~:text=process%20is%20a%20linearly%20transformed,unobservable%20state%20process%20with%20noise)). This procedure effectively performs a form of interpolation/regression using the state-space model structure. For example, if GDP is only reported quarterly (missing in between), a state-space model can still be run at a monthly frequency with GDP entries missing; the Kalman smoother will use the dynamics and other series (inflation, etc.) to infer those interim values ([
            Dynamic principal component analysis with missing values - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC9042056/#:~:text=process%20is%20a%20linearly%20transformed,unobservable%20state%20process%20with%20noise)). Studies have found that Kalman smoothing typically outperforms naive methods (like linear interpolation) for macroeconomic missing data imputation ([
            Dynamic principal component analysis with missing values - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC9042056/#:~:text=interpolation%2C%20Kalman%20smoothing%20of%20Durbin,unobservable%20state%20process%20with%20noise)) ([
            Dynamic principal component analysis with missing values - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC9042056/#:~:text=Results%20are%20listed%20in%20Table%C2%A01,smoothing%20and%20ISC%20algorithm%20is)).

In summary, Kalman filter techniques provide a principled model-based way to handle missing data: the filter naturally skips updates for missing observations ([5.5 Missing Data | A Very Short Course on Time Series Analysis](https://bookdown.org/rdpeng/timeseriesbook/missing-data.html#:~:text=Conceptually%2C%20missing%20data%20are%20easy,to%20the%20next%20time%20point)), and the smoother leverages all available data to impute gaps. The linear Gaussian assumptions make these calculations analytically tractable, and extensions via EM allow estimating unknown parameters despite missing observations ([An Approach To Time Series Smoothing And Forecasting Using The Em Algorithm](https://ideas.repec.org/a/bla/jtsera/v3y1982i4p253-264.html#:~:text=,likelihood%20estimators%20for%20the%20parameters)).

## Bayesian Methods

**Bayesian Imputation Framework:** Bayesian approaches treat both the model parameters and missing values as random variables with probability distributions. Instead of seeking single “best” estimates, we aim to compute the *posterior distribution* of the unknown quantities given the observed data. For a time series with observed values $y_{\text{obs}}$ and missing values $y_{\text{miss}}$, a Bayesian model might posit a joint prior $p(\theta, y_{\text{miss}})$ (where $\theta$ are model parameters or latent states) and a likelihood for $y_{\text{obs}}$. Inference is then based on the posterior $p(\theta, y_{\text{miss}} \mid y_{\text{obs}}) \propto p(y_{\text{obs}} \mid \theta, y_{\text{miss}})\,p(\theta, y_{\text{miss}})$. A key advantage is that instead of filling in a single guess for a missing value, we get a *distribution* over what that value could be, reflecting uncertainty. Bayesian imputation often leads to **multiple imputation**: drawing several plausible values for each missing entry from the posterior predictive distribution (rather than just one), which is useful for uncertainty quantification ([
            Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10529422/#:~:text=A%20static%20data%20set%20without,For%20the%20imputation%20process)).

If a state-space model is used, the Bayesian approach corresponds to a **Bayesian Dynamic Linear Model (DLM)**. In a DLM (West & Harrison framework), we put priors on the initial state (and often on parameters like $F, Q, R$ if unknown) and update these priors with data. The Kalman filter equations actually have a Bayesian interpretation: they yield the posterior mean and covariance of the state $x_t$ given data up to $t$ (assuming Gaussian priors). When no observation arrives at time $t$, the prior for $x_t$ (the prediction) simply carries over as the posterior ([5.5 Missing Data | A Very Short Course on Time Series Analysis](https://bookdown.org/rdpeng/timeseriesbook/missing-data.html#:~:text=Conceptually%2C%20missing%20data%20are%20easy,to%20the%20next%20time%20point)). Thus, a Bayesian DLM naturally “does nothing” at times of missing data, just as in the Kalman filter case – the posterior for $x_t$ given no new $y_t$ remains the prior ([smooth
](https://www.mathworks.com/help/econ/ssm.smooth.html#:~:text=,for%20period%20t%20are%20equivalent)). The difference is that in a fully Bayesian treatment, one could also carry uncertainty in model parameters and integrate over it.

For example, suppose we have a simple AR(1) model for a macroeconomic series $y_t$:
$$y_t = \phi\,y_{t-1} + \epsilon_t, \qquad \epsilon_t \sim N(0,\sigma^2).$$
In a Bayesian setup, we would put priors on $\phi$ and $\sigma^2$. If some $y_t$ are missing, we treat those as latent variables. We could directly sample the missing $y_t$ as part of the inference (see Monte Carlo methods below) or integrate them out. Bayesian inference yields a posterior distribution for $(\phi,\sigma^2)$ that accounts for the uncertainty due to missing values, and a predictive distribution for each missing $y_t$.

**Gaussian Processes for Time Series Imputation:** Gaussian Process (GP) models provide a non-parametric Bayesian approach to time series imputation. A GP can be thought of as a prior over functions $f(t)$; for any set of time points, the vector of function values has a joint multivariate normal distribution. If we assume $y_t = f(t) + \text{noise}$ with $f(t)\sim \mathcal{GP}(0, k(t,t'))$ for some covariance kernel $k$, then conditioning on observed data gives a Bayesian interpolation. A key property of Gaussians is that conditioning and marginalizing are analytically tractable. If $t_*$ is a time with a missing value and we have observations at times $t_1,\dots,t_n$, the **posterior** mean and variance for $y_{t_*}$ given the observed data $\mathbf{y}=(y_{t_1},\ldots,y_{t_n})^T$ are: 

- $\displaystyle E[y_{t_*}\mid \mathbf{y}] = K(t_*,\,T_{\text{obs}})\;[K(T_{\text{obs}},T_{\text{obs}}) + \sigma^2 I]^{-1}\,\mathbf{y},$ 
- $\displaystyle \Var(y_{t_*}\mid \mathbf{y}) = K(t_*,t_*) - K(t_*,\,T_{\text{obs}})\;[K(T_{\text{obs}},T_{\text{obs}})+\sigma^2 I]^{-1}\,K(T_{\text{obs}},t_*),$ 

where $K(t_*,T_{\text{obs}})$ is the covariance vector between $y_{t_*}$ and each observed $y_{t_i}$, and $K(T_{\text{obs}},T_{\text{obs}})$ is the covariance matrix of observed points (noise variance $\sigma^2$ is added if we model observation noise). This is the well-known GP predictive formula (similar to Kriging in geostatistics). It yields a normal distribution for the missing value. Notably, if there are multiple outputs (multivariate series), one can use a multi-output GP or separate GPs with correlations; but even treating each series separately, GPs *“easily ignore all positions that are neither observed nor queried”* – they simply marginalize them out ([](https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf#:~:text=Observing%20elements%20of%20the%20vector,are%20neither%20observed%20nor%20queried)). There is no need to explicitly impute in order to learn; the GP’s covariance function uses observed points to infer the mean and uncertainty at unobserved points.

In practice, GP imputation works well for capturing smooth trends in macroeconomic data (with an appropriate kernel), but can be computationally expensive if data are large. Still, it is a fully Bayesian approach since it produces a posterior distribution for missing values. It is also flexible: unlike a parametric state-space model, a GP can fit complex nonlinear patterns if the kernel is chosen or learned from data.

**Monte Carlo Methods (Gibbs Sampling):** When closed-form posteriors are not available, Bayesian imputation often relies on Monte Carlo simulation (MCMC). **Gibbs sampling** is particularly useful for missing data problems ([[Q] How to treat unbalanced Panel data with many missing values?](https://www.reddit.com/r/statistics/comments/18suqpv/q_how_to_treat_unbalanced_panel_data_with_many/#:~:text=,sophisticated%20approaches%20using%20Gibbs)). In a Gibbs sampler, we iteratively sample from conditional distributions. For example, in a time series with missing $y_t$, one can augment the model by treating $y_t$ as an unknown parameter. A Gibbs sampler would alternate between: (a) sampling the missing $y_t$ from its conditional distribution given the current guess of model parameters and the observed neighbors, and (b) sampling model parameters (or states) given a complete data series (where $y_t$ is now filled in). This is a form of **data augmentation**, where we “fill in” missing data during the MCMC and integrate them out by averaging over many samples ([Computational Statistics II - Unit C.1: Missing data problems, Gibbs sampling and the em algorithm](https://tommasorigon.github.io/CompStat/slides/un_C1.pdf#:~:text=If%20the%20observations%20were%20all,Bicocca%29%204%20%2F%2020)) ([Computational Statistics II - Unit C.1: Missing data problems, Gibbs sampling and the em algorithm](https://tommasorigon.github.io/CompStat/slides/un_C1.pdf#:~:text=Data%20augmentation%20and%20Gibbs%20sampling,natural%20choice%20in%20this%20setting)). For linear Gaussian models, one can combine forward-backward algorithms with Gibbs: for instance, a **Forward-Filtering Backward-Sampling (FFBS)** algorithm first runs a Kalman filter forward, then *samples* a random state trajectory $x_{1:T}$ backward in one sweep ([Particle Learning and Smoothing - Project Euclid](https://projecteuclid.org/journals/statistical-science/volume-25/issue-1/Particle-Learning-and-Smoothing/10.1214/10-STS325.pdf#:~:text=Particle%20Learning%20and%20Smoothing%20,are)). Any missing observations can be drawn as $y_t \sim N(H\hat{x}_{t|T}, R_t)$ conditional on the sampled state. This Bayesian simulation smoother (Carter and Kohn, 1994) effectively imputes missing values in each Monte Carlo draw, yielding a set of plausible trajectories for the series.

To illustrate, consider a simplified scenario: $y_t = \phi y_{t-1} + \epsilon_t$ with unknown $\phi$ and missing some $y_t$. A Gibbs sampler might do:
1. **Sample missing $y_t$:** For each missing time, draw $y_t^{(new)}$ from $N(\phi^{(old)}(y_{t-1} + y_{t+1})/2,\;\sigma^2/2)$ – this is the conditional distribution given adjacent points (details depend on model and boundary conditions).
2. **Sample parameter $\phi$:** Given a full series (with imputed values), sample $\phi$ from its posterior (e.g. if we put a prior on $\phi$, combine with likelihood of an AR(1) on the now-complete series).
3. Repeat. 

This will produce a sequence of $(\phi, y_{\text{miss}})$ draws from the joint posterior. From those, we can compute posterior means or predictive intervals for the missing values. The Gibbs approach is general and works even for models where Kalman filter formulas break down (e.g. non-linear or non-Gaussian models), albeit at higher computational cost.

**Bayesian Dynamic Linear Models (DLMs):** A Bayesian DLM extends the state-space model by treating parameters as random. For example, in a macroeconomic context, one might use a DLM with time-varying regression coefficients or stochastic volatility. Missing data are handled by the filtering distribution: if $y_t$ is missing, the *prior* for the state at $t$ becomes the *posterior* (since likelihood is absent) – the sequential Bayesian update is essentially $\text{Posterior}_{t} = \text{Prior}_{t}$ in that case. Bayesian DLMs often use **prior distributions** for $F, Q, H, R$ and then use MCMC to sample these along with the states. Alternatively, one can apply variational Bayesian methods or particle filters for sequential Bayesian learning with missing data.

One benefit of Bayesian DLMs is coherent uncertainty quantification. For instance, if GDP is missing for a period, a Bayesian model can provide a predictive distribution for GDP in that period (combining information from other series and dynamics) instead of a single imputed value. By drawing multiple samples from that predictive distribution (multiple imputations), one can propagate the uncertainty of the missing GDP into downstream analyses ([
            Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10529422/#:~:text=A%20static%20data%20set%20without,For%20the%20imputation%20process)).

In summary, Bayesian methods view missing time series values as additional unknowns to be inferred. Whether through analytic GP formulas or simulation-based algorithms, they produce a full distribution for missing values. This is particularly valuable in macroeconomic settings to reflect the confidence (or lack thereof) in imputed figures. Bayesian DLMs, in particular, unify the state-space approach with Bayesian inference: when analytical filtering is possible, they coincide with Kalman filtering for updating, but also allow incorporating prior knowledge and parameter uncertainty. Modern Bayesian software (e.g. Stan, PyMC) can handle quite complex time series models with missing data by automatically performing MCMC on augmented parameter spaces.

## Neural Network Techniques

**Autoencoder-Based Imputation:** Autoencoders are neural networks trained to reconstruct their inputs, often through a lower-dimensional bottleneck. For missing data imputation, a popular strategy is the **denoising autoencoder (DAE)**: we intentionally corrupt or mask parts of the input and train the network to recover the original complete data. In the context of time series, one can treat missing positions as corrupted inputs. The autoencoder (with parameters $\theta$) defines a mapping $f_\theta: \mathbf{y}_{\text{in}} \to \hat{\mathbf{y}}_{\text{out}}$, where $\hat{\mathbf{y}}$ is the reconstruction. During training, we take a complete time series sample $\mathbf{y}$, artificially mask some values (or add noise), and feed this $\mathbf{y}_{\text{mask}}$ into the autoencoder, using the original $\mathbf{y}$ as the target. The loss might be, for example, the mean squared error on the masked entries:
$$L(\theta) = \mathbb{E}_{\text{mask}}\sum_{t: y_t \text{ masked}}(y_t - \hat{y}_t)^2.$$ 
By minimizing this, the autoencoder learns to *infer missing values from the context of other variables and time points*. Essentially, it learns the nonlinear correlations in the data. Researchers have proposed stacking multiple autoencoder layers (SDAE) or using variants like **variational autoencoders (VAE)** for imputation. A VAE, for instance, can learn a probabilistic latent space for the time series and sample imputations from the learned distribution.

Autoencoders have shown promise in capturing complex patterns in macroeconomic data that simpler linear models might miss. For example, an autoencoder can learn that inflation and unemployment tend to move in opposite directions (Phillips curve), and use one to help impute missing values in the other. In one deep imputation model (MIDIA), a *“Missing Data Imputation denoising Autoencoder”* was trained to explore non-linear relationships between missing and observed values ([MIDIA: exploring denoising autoencoders for missing data imputation | OpenReview](https://openreview.net/forum?id=iSKCbHoe9o#:~:text=explore%20the%20correlations%20amongst%20the,independent%20MIDIA%20model%20for%20each)). By learning from many instances of partially observed data, the autoencoder develops an internal model of the time series structure and can fill in gaps accordingly.

One challenge is that autoencoders typically require i.i.d. training examples; for a single long time series, one might train by taking sliding windows as training samples. Alternatively, in a multivariate series setting, each time step (with the vector of variables) could be an training sample for a denoising autoencoder (masking some variables at each step). There are also hybrid approaches where an autoencoder is combined with a recurrence to handle temporal aspect (see below). 

Mathematically, a simple autoencoder for imputation might operate as $\hat{\mathbf{y}} = g_{\text{dec}}(h)$ where $h = g_{\text{enc}}(\mathbf{y}_{\text{mask}})$ is a compressed representation from an encoder. The network is forced to reconstruct missing components of $\mathbf{y}$. It essentially performs a form of nonlinear regression imputation. Empirical studies find that DAEs can outperform traditional imputation (like mean or MICE) especially when relationships are highly nonlinear ([MIDIA: exploring denoising autoencoders for missing data imputation | OpenReview](https://openreview.net/forum?id=iSKCbHoe9o#:~:text=training%20a%20uniform%20MIDIA%20model,data%20with%20general%20missing%20pattern)). However, they require a lot of data to train and careful regularization to generalize well.

**Recurrent Neural Networks (RNNs) and LSTMs:** Recurrent neural networks are suited for sequential data and can naturally handle time series with arbitrary length. Standard RNNs (or LSTM/GRU networks) can be adapted for missing data in a few ways:

- **Feeding Mask Indicators:** The network can be given, at each time $t$, not only the value of each variable but also a binary indicator of whether it was observed or missing. For example, one can input $(y_t, m_t)$ where $m_t$ is a mask vector (with $m_{t,i}=1$ if $y_{t,i}$ is observed and 0 if missing). The RNN can learn to ignore or down-weight missing inputs. This approach was used in models like GRU-D, which augments a GRU cell with decay terms for missing inputs ([
            Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10529422/#:~:text=sets,imputation%20methods%20based%20on%20three)). GRU-D introduces a decay function $\gamma(\Delta t)$ that exponentially decays the last observed value of a feature over time $\Delta t$ of missingness, so the hidden state uses an imputed value that regresses to the mean as missing duration grows. Formally, if $z_{t}$ is a feature that has a missing gap since last observed at time $t_0$, GRU-D uses $\tilde{z}_{t} = m_{t} z_{t} + (1-m_{t})(\gamma(\Delta t) \, z_{t_0} + (1-\gamma(\Delta t))\,\bar{z})$, where $\bar{z}$ is some global mean. This way, the RNN input uses either the true value (if present) or a decayed estimate if absent ([
            Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10529422/#:~:text=BRITS%20is%20superior%20to%20RNN,nearest%20neighbor%20%28KNN%29%20methods)). The RNN’s gates also get $m_t$ and $\Delta t$ as inputs.

- **Direct Imputation with RNN:** Another powerful approach is to **treat missing values as learnable parameters** and use the RNN’s loss to solve for them. The BRITS model (Bidirectional Recurrent Imputation for Time Series) does this: it sets up a forward RNN and a backward RNN that process the time series, and defines the missing values such that they minimize the overall loss in both directions ([[PDF] BRITS: Bidirectional Recurrent Imputation for Time Series - arXiv](https://arxiv.org/pdf/1805.10572#:~:text=arXiv%20arxiv,values%20with%20bidirectional%20recurrent%20dynamics)). In BRITS, a missing $y_{t,i}$ is initialized (e.g. to 0) and during training, gradients flow into it as if it were a network weight. The RNN essentially “fills” the value that best fits the temporal dynamics. This method imposes *no explicit statistical assumption* on the missing pattern aside from the RNN’s learned dynamics. As described by Cao et al., *“the imputed values are treated as variables of the RNN computational graph and can be effectively updated during backpropagation”* ([BRITS: Bidirectional Recurrent Imputation for Time Series](http://papers.neurips.cc/paper/7911-brits-bidirectional-recurrent-imputation-for-time-series.pdf#:~:text=bidirectional%20recurrent%20dynamical%20system%2C%20without,Experiments)). The model trains by minimizing a reconstruction loss (differences between predicted and actual values for observed entries) plus maybe a smoothing regularizer. By running forward and backward (which uses future data as well), BRITS ensures consistency and uses all information to determine missing values. Empirically, BRITS was shown to outperform earlier RNN-based imputers like GRU-D and M-RNN ([
            Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10529422/#:~:text=BRITS%20is%20superior%20to%20RNN,nearest%20neighbor%20%28KNN%29%20methods)).

- **Sequence-to-Sequence Models:** One can also set up a seq2seq model where the input is a time series with gaps and the output is the same series with gaps filled. During training, random gaps are introduced and the network is trained to predict the original full sequence. This is analogous to a denoising autoencoder but with an RNN architecture. In practice, combining convolutional layers (for local patterns) with RNNs for global sequence patterns can improve performance for long series imputation.

RNN-based approaches can capture complex temporal dependencies (seasonality, lags, regime changes) that static methods might miss. LSTM units are particularly adept at remembering long-term information, which is useful if a macroeconomic series has a long trend that can inform a future missing value. One must be careful to avoid the network simply learning to copy last seen values — which is why bidirectional training or artificial masking during training (to force the network to learn interpolation, not just persistence) is important.

**Transformer-Based Models:** Transformers have recently been applied to time series imputation with great success. Transformers rely on self-attention mechanisms that can capture long-range dependencies without recursion. For missing data, a baseline approach is to use a **mask tokens** or indicators for missing positions and train the transformer in a self-supervised way to predict those masked values (analogous to BERT in NLP). 

One example is the **Multivariate Time-Series Imputation with Transformers (MTSIT)** model ([Multivariate Time Series Imputation with Transformers | CoLab](https://colab.ws/articles/10.1109%2Flsp.2022.3224880#:~:text=medicine%2C%20and%20economics,Experiments%20show%20that%20MTSIT%20outperforms)). It uses only the encoder part of the transformer to avoid complexity, and jointly trains on two tasks: reconstructing the original input series from a randomly masked version, and imputing actual missing values. Specifically, random subsets of values are masked during training, and the transformer encoder is tasked with outputting the complete series. The loss is computed on both the artificially masked positions (to train the model) and any true missing positions (if we integrate the imputation task into training). The model uses positional encoding to maintain temporal order, and self-attention allows it to attend to all other time points (or a neighborhood, if constrained) when estimating a missing point. Because the transformer does not assume a fixed time step dependency (unlike RNNs that propagate sequentially), it can naturally leverage any observed value in the series, no matter how far apart in time, to inform an imputation. This is particularly useful in macroeconomic data where observations like GDP (quarterly) might rely on distant past values or other series in a non-local way.

The MTSIT approach was shown to **outperform state-of-the-art methods** on benchmark datasets ([Multivariate Time Series Imputation with Transformers | CoLab](https://colab.ws/articles/10.1109%2Flsp.2022.3224880#:~:text=propose%20the%20Multivariate%20Time,imputation%20methods%20over%20benchmark%20datasets)). A related model, SAITS (Self-Attention Imputation for Time Series), introduced an even more efficient architecture by simplifying the attention layers while maintaining performance, noting that a tailored self-attention can outperform a vanilla Transformer for imputation ([[PDF] saits: self-attention-based imputation for time series - arXiv](https://arxiv.org/pdf/2202.08516#:~:text=%5BPDF%5D%20saits%3A%20self,To%20further%20compare%20the)). The general training objective for such models often includes a combination of reconstruction loss (for artificially masked data) and imputation loss (for real missing data) ([](https://arxiv.org/pdf/2202.08516#:~:text=L%20%3D%20LORT%20%2B%20%CE%BBLMIT,We%20let%20X%CB%9C)) ([](https://arxiv.org/pdf/2202.08516#:~:text=three%2C%20i,minimizing%20the%20final%20loss%20L)).

In practice, using transformers for imputation involves constructing an input with placeholders for missing values and a mask matrix $M$ indicating missing entries. The transformer outputs a prediction $\hat{Y}$ for all time steps and variables. The network is trained to minimize $||M \odot (Y - \hat{Y})||^2$ (error only on observed positions) during training, plus any additional masked-signal losses. At inference, we input the series with $M$ for known parts and let the model generate $\hat{Y}$ for missing parts. The attention mechanism ensures that $\hat{Y}_{t,i}$ (say GDP at time $t$ missing) is computed as a weighted average of other available values (across time and variables) learned by the model. This can capture, for example, that “GDP at time $t$ is usually similar to GDP at time $t-1$ and $t+1$, and also correlated with employment at time $t$,” all learned from data rather than imposed.

**Hybrid Approaches (Neural Networks + Filtering):** Combining neural networks with statistical filters can leverage the strengths of both. One way is to use a neural network to *learn the dynamics or observation function* in a state-space model, and then apply a Kalman-like filter. For example, a neural network can learn a nonlinear state transition $x_{t+1} = f_\theta(x_t)$ from data, while Gaussian noise assumptions allow a Kalman filter (extended or unscented) to do the state estimation. This is a **Neural State-Space Model** approach. Alternatively, one might use a Kalman filter to produce interim smoothed estimates, and feed those as features into a neural network that learns the residual structure. A concrete example: in an environmental application, a **hybrid Extended Kalman Filter** was augmented by a feedforward neural network to adjust for bias in a physics-based model ([A Hybrid Extended Kalman Filter Based on a Parametrized FeedForward Neural Network for the Improvement of the Results of Numerical Wave Prediction Models](https://www.mdpi.com/2673-4931/26/1/199#:~:text=systems,the%20model%20and%20the%20actual)) ([A Hybrid Extended Kalman Filter Based on a Parametrized FeedForward Neural Network for the Improvement of the Results of Numerical Wave Prediction Models](https://www.mdpi.com/2673-4931/26/1/199#:~:text=Under%20this%20framework%2C%20we%20introduce,and%20the%20optimal%20value%20is)). The neural net was trained to estimate the systematic error (bias) given the model’s output, and the EKF incorporated this correction, leading to improved predictions.

In the context of macroeconomic data, a hybrid approach might involve using an RNN to model cyclical patterns or regime changes, but within each regime use a Kalman filter to handle noise and missing data. Another example is **KalmanNet**, which inserts a small RNN into the classical Kalman filtering equations to handle partially known or nonlinear systems ([](https://pure.tue.nl/ws/files/201346390/KalmanNet_Neural_Network_Aided_Kalman_Filtering_for_Partially_Known_Dynamics.pdf#:~:text=Abstract%E2%80%94State%20estimation%20of%20dynamical%20systems,network%20module%20in%20the%20flow)). KalmanNet learns to compute the Kalman gain $K_t$ in situations where the model ($F, H, Q, R$) is not fully known or linear, essentially learning to “filter” in a data-driven way ([](https://pure.tue.nl/ws/files/201346390/KalmanNet_Neural_Network_Aided_Kalman_Filtering_for_Partially_Known_Dynamics.pdf#:~:text=Here%2C%20we%20present%20KalmanNet%2C%20a,both%20mismatched%20and%20accurate%20domain)). This approach retains the interpretability of a filtering framework (with state estimates and uncertainty) while using neural networks to compensate for model misspecifications.

Hybrid methods can also mean **two-stage models**: e.g. first use a Kalman smoother to fill missing data (because it gives principled estimates and uncertainty), then train a neural network forecaster on the completed data; or conversely, use a neural network to fill gaps (perhaps capturing nonlinear patterns) and then apply a structural time series model on the imputed series to leverage economic interpretation. In academic research, such hybrids are promising because they combine **model-driven** and **data-driven** approaches. For instance, a hybrid algorithm might *“use the Kalman filter to smooth the data and a neural network to learn a distance model,”* achieving lower errors than either alone ([A review: state estimation based on hybrid models of Kalman filter ...](https://www.tandfonline.com/doi/full/10.1080/21642583.2023.2173682#:~:text=,error%20and%20improve%20the%20accuracy)).

---

**Conclusion:** Each of the above techniques has trade-offs. **Kalman filtering methods** rely on an explicit state-space model, offering transparency and statistical optimality under Gaussian assumptions; they handle missing data elegantly through the prediction step ([5.5 Missing Data | A Very Short Course on Time Series Analysis](https://bookdown.org/rdpeng/timeseriesbook/missing-data.html#:~:text=Conceptually%2C%20missing%20data%20are%20easy,to%20the%20next%20time%20point)) and can be extended via EM for parameter learning ([An Approach To Time Series Smoothing And Forecasting Using The Em Algorithm](https://ideas.repec.org/a/bla/jtsera/v3y1982i4p253-264.html#:~:text=,likelihood%20estimators%20for%20the%20parameters)). **Bayesian methods** provide a full probabilistic treatment, allowing practitioners to quantify uncertainty in imputations and incorporate prior knowledge; they are computationally heavier but very flexible, accommodating nonlinear models via MCMC and providing principled multiple imputations. **Neural network approaches** (autoencoders, RNNs, Transformers) are purely data-driven and can capture complex nonlinear and long-range patterns in macroeconomic series that classical models might miss. They often achieve high imputation accuracy ([BRITS: Bidirectional Recurrent Imputation for Time Series](http://papers.neurips.cc/paper/7911-brits-bidirectional-recurrent-imputation-for-time-series.pdf#:~:text=updated%20during%20backpropagation,methods%20in%20both%20imputation%20and)) ([Multivariate Time Series Imputation with Transformers | CoLab](https://colab.ws/articles/10.1109%2Flsp.2022.3224880#:~:text=propose%20the%20Multivariate%20Time,imputation%20methods%20over%20benchmark%20datasets)), but require large data and careful tuning, and may lack the interpretability of model-based methods. Hybrid strategies seek the best of both worlds by embedding domain-specific structure into neural networks or vice versa.

In practice, for macroeconomic time series, one might choose a method based on the data at hand and the analysis goals: Kalman smoothing (possibly with a structural model) if one trusts a specified economic model and wants interpretable results; Bayesian imputation if assessing uncertainty is paramount; or a learning-based method (like a Transformer or RNN) if the goal is predictive accuracy and the relationships are too complex to specify manually. Each approach, however, relies on a solid mathematical foundation for handling missing data – whether it’s the linear algebra of Kalman filters or the loss functions of deep networks – to ensure that the imputed values are as sound and informative as possible. 

**Sources:**

- Kalman filter handling of missing observations ([5.5 Missing Data | A Very Short Course on Time Series Analysis](https://bookdown.org/rdpeng/timeseriesbook/missing-data.html#:~:text=Conceptually%2C%20missing%20data%20are%20easy,to%20the%20next%20time%20point)) ([smooth
](https://www.mathworks.com/help/econ/ssm.smooth.html#:~:text=,for%20period%20t%20are%20equivalent)); state-space formulation ([How to handle incomplete data in Kalman Filter? - Cross Validated](https://stats.stackexchange.com/questions/86332/how-to-handle-incomplete-data-in-kalman-filter#:~:text=%24%24%20%5Cbegin,align%7D)); Kalman smoother imputation ([
            Dynamic principal component analysis with missing values - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC9042056/#:~:text=process%20is%20a%20linearly%20transformed,unobservable%20state%20process%20with%20noise)); EM algorithm for time series ([An Approach To Time Series Smoothing And Forecasting Using The Em Algorithm](https://ideas.repec.org/a/bla/jtsera/v3y1982i4p253-264.html#:~:text=,likelihood%20estimators%20for%20the%20parameters)).  
- Bayesian dynamic models and GP interpolation ([](https://www.cs.toronto.edu/~hinton/csc2515/notes/gp_slides_fall08.pdf#:~:text=Observing%20elements%20of%20the%20vector,are%20neither%20observed%20nor%20queried)); data augmentation and Gibbs sampling ([Computational Statistics II - Unit C.1: Missing data problems, Gibbs sampling and the em algorithm](https://tommasorigon.github.io/CompStat/slides/un_C1.pdf#:~:text=If%20the%20observations%20were%20all,Bicocca%29%204%20%2F%2020)) ([Particle Learning and Smoothing - Project Euclid](https://projecteuclid.org/journals/statistical-science/volume-25/issue-1/Particle-Learning-and-Smoothing/10.1214/10-STS325.pdf#:~:text=Particle%20Learning%20and%20Smoothing%20,are)).  
- Autoencoder and RNN imputation methods ([MIDIA: exploring denoising autoencoders for missing data imputation | OpenReview](https://openreview.net/forum?id=iSKCbHoe9o#:~:text=explore%20the%20correlations%20amongst%20the,independent%20MIDIA%20model%20for%20each)) ([BRITS: Bidirectional Recurrent Imputation for Time Series](http://papers.neurips.cc/paper/7911-brits-bidirectional-recurrent-imputation-for-time-series.pdf#:~:text=bidirectional%20recurrent%20dynamical%20system%2C%20without,Experiments)); performance of BRITS vs others ([
            Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking - PMC
        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC10529422/#:~:text=BRITS%20is%20superior%20to%20RNN,nearest%20neighbor%20%28KNN%29%20methods)); Transformer-based imputation ([Multivariate Time Series Imputation with Transformers | CoLab](https://colab.ws/articles/10.1109%2Flsp.2022.3224880#:~:text=medicine%2C%20and%20economics,Experiments%20show%20that%20MTSIT%20outperforms)); hybrid Kalman filter + NN approaches ([A Hybrid Extended Kalman Filter Based on a Parametrized FeedForward Neural Network for the Improvement of the Results of Numerical Wave Prediction Models](https://www.mdpi.com/2673-4931/26/1/199#:~:text=systems,the%20model%20and%20the%20actual)) ([](https://pure.tue.nl/ws/files/201346390/KalmanNet_Neural_Network_Aided_Kalman_Filtering_for_Partially_Known_Dynamics.pdf#:~:text=Abstract%E2%80%94State%20estimation%20of%20dynamical%20systems,network%20module%20in%20the%20flow)).